{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/adam/Documents/spark/sp\")\n",
    "import pyspark \n",
    "findspark.find()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Instanciation de client Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession\\\n",
    "                  .builder.master(\"local[3]\")\\\n",
    "                  .appName(\"Kmeans_Clustering\")\\\n",
    "                  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Création d un fichier properties.conf contenant les informations relatives à vos paramètres du programme en dur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['properties.conf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('properties.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation du fichier de configuration pour récupérer les path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_input_data   = config['Bristol-City-bike']['Input-data']\n",
    "path_to_output_data  = config['Bristol-City-bike']['Output-data']\n",
    "num_partition_kmeans = config['Bristol-City-bike']['Kmeans-level'] \n",
    "num_partition_kmeans  = int(num_partition_kmeans )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Importer le json avec spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bristol = spark.read.json(path_to_input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des deux premier ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------------+------+\n",
      "|             address|  latitude| longitude|                name|number|\n",
      "+--------------------+----------+----------+--------------------+------+\n",
      "|Lower River Tce /...|-27.482279|153.028723|122 - LOWER RIVER...|   122|\n",
      "|Main St / Darragh St| -27.47059|153.036046|91 - MAIN ST / DA...|    91|\n",
      "+--------------------+----------+----------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bristol.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- création d un nouveau data frame Kmeans-df contenant seulement les variables latitude et longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_df=bristol.select(\"latitude\",\"longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Affichage des premier ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  latitude| longitude|\n",
      "+----------+----------+\n",
      "|-27.482279|153.028723|\n",
      "| -27.47059|153.036046|\n",
      "+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Kmeans_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- k means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "features = ('longitude','latitude')\n",
    "kmeans = KMeans().setK(num_partition_kmeans).setSeed(1)\n",
    "assembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n",
    "dataset=assembler.transform(Kmeans_df)\n",
    "model = kmeans.fit(dataset)\n",
    "fitted = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Noms des colonnes de fitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude', 'longitude', 'features', 'prediction']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7- Détermination des longitudes et des latitudes moyennes pour chaque groupe en utilisant spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+\n",
      "|prediction|      avg(latitude)|    avg(longitude)|\n",
      "+----------+-------------------+------------------+\n",
      "|         1|-27.460240636363633|153.04186302272726|\n",
      "|         2| -27.47255990624999|   153.02594553125|\n",
      "|         0|-27.481218536585374|153.00572882926832|\n",
      "+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moy_DSL=fitted\\\n",
    "               .groupBy(\"prediction\")\\\n",
    "               .mean(\"latitude\",\"longitude\")\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+\n",
      "|prediction|       Moy_Latitude|     Moy_Longitude|\n",
      "+----------+-------------------+------------------+\n",
      "|         1|-27.460240636363633|153.04186302272726|\n",
      "|         2| -27.47255990624999|   153.02594553125|\n",
      "|         0|-27.481218536585374|153.00572882926832|\n",
      "+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted.createOrReplaceTempView(\"fittedSQL\") # transformation du data frame en table !\n",
    "spark.sql(\"\"\" select prediction, \n",
    "                 mean(latitude) as Moy_Latitude,\n",
    "                 mean(longitude) as Moy_Longitude\n",
    "           from fittedSQL\n",
    "           group by prediction   \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9- Exportation de la data frame fitted après élimination de la colonne  features, dans le répertoire path-to-output-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = fitted.drop('features')\n",
    "fitted = fitted.write.format(\"csv\").mode(\"overwrite\")\\\n",
    "        .save(path_to_output_data, header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
